<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Jin Saeki Ko</title>
<link>https://jinsko.com/blog.html</link>
<atom:link href="https://jinsko.com/blog.xml" rel="self" type="application/rss+xml"/>
<description>A website for thinking.</description>
<generator>quarto-1.8.26</generator>
<lastBuildDate>Sun, 21 Dec 2025 00:00:00 GMT</lastBuildDate>
<item>
  <title>SPICES Model of Medical Education</title>
  <link>https://jinsko.com/blog/spices.html</link>
  <description><![CDATA[ 




<p>I enjoy reading about medicine. I figured I should read about how medicine should be learnt. What follows is an argument for the more liberal forms of the SPICES model of medical education <span class="citation" data-cites="hardenEducationalStrategiesCurriculum1984">(Harden, Sowden, and Dunn 1984)</span>. The paper itself is relatively old, but many medical schools, especially in the States, have still not seriously re-evaluated their curriculum. Even though newer schools tend to be more liberal in their approach, the inertia of older programs makes the framework oddly current, and therefore still relevant.</p>
<p>The SPICES model is a way of describing a curriculum by placing it on six continua. The acronym stands for student-centred versus teacher-centred learning, problem-based versus information-gathering learning, integrated versus discipline-based teaching, community-based versus hospital-based education, electives versus a standard programme, and systematic versus apprenticeship or opportunistic training. The point is not that every school must live at one extreme, but that schools should be honest about where they sit and whether that position makes sense.</p>
<section id="student-centered-vs-teacher-centered-learning" class="level3">
<h3 class="anchored" data-anchor-id="student-centered-vs-teacher-centered-learning">Student-centered vs Teacher-centered Learning</h3>
<p>Student-centered learning makes more sense to me. In its simplest form, it means the student has real responsibility over what gets learned and how, rather than acting as a passive receiver of whatever the instructor decides to deliver. It shifts the emphasis from teaching as performance to learning as an activity. I may not always wish to be taught, but I do wish to learn; the paper referenced a line like this attributed to Winston, and it captures the difference well. What matters in education is what the students learn, not what the teacher teaches. If you actually believe that, then you should prefer a model that makes the learner do the work of learning.</p>
<p>The arguments for the student-centred approach are not complicated. It makes learning active rather than passive, and it trains the habit that matters most in medicine: the ability to identify what you don’t know and then go out of your way to fix it. It also allows the student to learn in a way that can actually be efficient for them rather than being forced into a one-size-fits-all method that cannot be guaranteed valuable on the instructor side. Besides, learning on your own is simply more fun. You get to do it your own way, and you get to chase coherence instead of pretending that compliance is understanding.</p>
<p>The objections to student-centred learning tend to be weak. One objection is that many teachers have only experienced a teacher-centred approach. My objection is that this is not a valid argument against the principles of student-centred learning. It’s an argument that the familiar approach is convenient, not that it is better. Education should never be treated with such obstinance. Another objection is that students may be apprehensive because their previous experience was teacher-centred. My objection is harsher: if you’ve never gone out of your way to learn on your own and you feel threatened by the idea, in your lack of curiosity, you don’t deserve to be educated. Deal with it. Physicians have a moral obligation to learn for their patients without being spoon-fed—keeping up with literature, relearning what time has eroded, and updating what evidence overturns. If you can’t do that, don’t become a physician. Student-centred learning emphasizes exactly this form of active learning, and that is why I argue for it very liberally.</p>
</section>
<section id="problem-based-learning-vs-information-gathering-model" class="level3">
<h3 class="anchored" data-anchor-id="problem-based-learning-vs-information-gathering-model">Problem-based Learning vs Information-gathering Model</h3>
<p>I also argue for problem-based learning over the information-gathering model. Problem-based learning means starting with a problem or condition and using that as the engine that determines what you need to learn, so that basic sciences are pulled in as tools for explanation. The information-gathering model goes the other way: you accumulate foundational facts first, usually sorted by discipline, and only later apply them to conditions. When problem solving in the hospital or clinic, clinical reasoning should trace a path that begins with manifestations—lab tests, symptoms, radiology—and then moves toward potential causes grounded in the basic sciences. Manifestation leads you to mechanism. That aligns naturally with problem-based learning, where you meet the condition first and then learn the causes through a deeper dive, which forces the foundational sciences to become meaningful rather than decorative.</p>
<p>The information-gathering model feels backwards to me. It moves from basic sciences to conditions and asks the student to trust that relevance will appear later. I don’t understand the logic behind this as a default. It feels like a convenient and lossy curriculum derived from textbooks that are demarcated by subject matter, rather than an attempt at an integrated and coherent curriculum. Problem-based learning also helps eliminate superfluous information, because every piece of foundational science you learn should be required for a full explanation. It is far too common within education to learn unnecessary information simply because it exists, not because it is needed.</p>
<p>The common objections to problem-based learning are also predictable. One is that you need a solid foundation first, and vocabulary matters, as emphasized in the information-gathering model. I don’t understand why you can’t do this within problem-based learning. If you dive deep enough into any clinical problem, you will always find the foundations, and you will learn more than enough to explain pathological phenomena. And vocabulary is easy. Learn the medical Latin and Greek roots and you’re good; it isn’t too difficult to intuit what a structure or process is once you break the word into its component roots. Another objection is that problem-based learning places a heavy load on instructors because it requires well-prepared, integrated material aligned with problem solving. That’s understandable, but it’s not an argument against the principles of problem-based learning. It’s an argument for the avoidance of such practices, which is a different thing.</p>
</section>
<section id="integreated-vs-discipline-based-teaching" class="level3">
<h3 class="anchored" data-anchor-id="integreated-vs-discipline-based-teaching">Integreated vs Discipline-based Teaching</h3>
<p>In the same vein, I argue for integrated teaching rather than discipline-based teaching. Discipline-based teaching segments medicine into separate subjects—anatomy here, physiology there, pathology somewhere else—and then expects the student to assemble the overall picture on their own. Integrated teaching attempts to interrelate these areas deliberately so that the student is not forced to do all the synthesis after the fact. Integration can be horizontal, meaning integration across disciplines at the same stage, like learning anatomy and physiology together within a system, or vertical, meaning integration across time, linking basic sciences with clinical thinking rather than pretending basic science ends when the clinic begins. The integrated approach aligns with problem solving because adequate explanations for real conditions usually require multiple disciplines at once. The body does not respect departmental boundaries, so education should stop acting like it does.</p>
<p>One of the better points the paper makes is that integration promotes communication between instructors across disciplines. It forces them to think about the overall aims of the curriculum and of the school rather than being limited to their own departments. That shift matters. Departments have incentives to defend their subject-matter territory, but students need coherent models, not territorial claims. Integration is one way to force the curriculum to admit that coherence is the actual goal.</p>
<p>There is, however, one objection to integration that I take seriously: it can be lossy. Since the integrated approach focuses less on the individual subjects and argues for a holistic approach, some information is bound to be lost. Whether that loss is dangerous is another debate, but the possibility is real. The omission of topics must be closely monitored, because “integration” is not a license to be sloppy. Another objection is that teachers are often more passionate about their own subjects, which could decrease enthusiasm in an integrated structure. Even if that happens, instructors should still be able to see the overlap between their own field and other fields, and they should be capable of teaching their piece as part of a whole. Another objection is that integration may induce uncertainty about which specialty a student should choose. I don’t understand the basis of this argument. Specialty choice is not based on book learning alone; it is shaped by experiences during rotations, mentorship, temperament, and the realities of daily work.</p>
</section>
<section id="community-based-vs-hospital-based" class="level3">
<h3 class="anchored" data-anchor-id="community-based-vs-hospital-based">Community-based vs Hospital-based</h3>
<p>I argue for the community-based approach over the hospital-based approach, but this is simpler than the other debates. Hospital-based education concentrates pathology and acute care, but community-based education is closer to the actual distribution of medicine: prevention, chronic disease, early presentation, follow-up, and the social context that determines whether treatment matters. If you train only in hospitals, you risk confusing the most intense slice of medicine for the whole of medicine. Community-based education corrects that distortion.</p>
</section>
<section id="elective-vs-uniform" class="level3">
<h3 class="anchored" data-anchor-id="elective-vs-uniform">Elective vs Uniform</h3>
<p>I also argue briefly for an elective-based curriculum. Electives add an element of responsibility and ownership, and they acknowledge that students do not all need the same depth in the same places at the same time. They allow exploration and the pursuit of specific interests without pretending that a uniform programme can optimize for every future physician. A standard core is necessary, but a purely standard programme assumes a single path, and medicine does not reward that assumption.</p>
</section>
<section id="systematic-vs-apprenticeship" class="level3">
<h3 class="anchored" data-anchor-id="systematic-vs-apprenticeship">Systematic vs Apprenticeship</h3>
<p>Finally, I argue for a systematic approach over the apprenticeship or opportunistic approach, because uniform coverage matters. Opportunistic learning depends on chance: which patients arrive, which cases happen to be on the ward, which attending happens to teach, and how service pressures distort teaching. A systematic curriculum is a commitment to ensuring that required experiences and competencies are covered rather than left to luck. In real medical settings, gaps are not just personal deficiencies; they become risks that bleed into patient care. If education is supposed to be ethical, then it cannot rely on accident.</p>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-hardenEducationalStrategiesCurriculum1984" class="csl-entry">
Harden, R. M., Susette Sowden, and W. R. Dunn. 1984. <span>“Educational Strategies in Curriculum Development: The <span>SPICES</span> Model.”</span> <em>Medical Education</em> 18 (4): 284–97. <a href="https://doi.org/10.1111/j.1365-2923.1984.tb01024.x">https://doi.org/10.1111/j.1365-2923.1984.tb01024.x</a>.
</div>
</div></section></div> ]]></description>
  <category>Medicine</category>
  <guid>https://jinsko.com/blog/spices.html</guid>
  <pubDate>Sun, 21 Dec 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Interbacterial Antagonism via Secretion Systems and its Applications</title>
  <link>https://jinsko.com/blog/sec_systems.html</link>
  <description><![CDATA[ 




<p>Most antagonism doesn’t need contact. Bacteria just secrete stuff into the environment—for instance, specific bacteriocins and broad antibiotics. These diffuse and hit any nearby cells that don’t have the right resistance or immunity. This is a great example of long-range killing, with no direct contact needed <span class="citation" data-cites="willeyPrescottsMicrobiology2020">(Willey et al. 2020)</span>.</p>
<p><strong>Type V secretion system:</strong> the cell has a huge outer-membrane protein, CdiA, exported by CdiB. The N-terminal part of CdiA binds a specific receptor on the target cell. The C-terminal part (CdiA-CT) is the toxin domain that actually goes into the target and does the damage (DNase, RNase, translation block, etc.). The producing cell makes a small immunity protein, CdiI, that binds its own CdiA-CT and blocks it. So, if you have the matching <em>cdiI</em> gene, you’re safe; if not, you get killed <span class="citation" data-cites="kleinContactDependentInterbacterialAntagonism2020">(Klein, Ahmad, and Whitney 2020)</span>.</p>
<p><strong>Type VI secretion system:</strong> basically a phage-like tube and sheath built into the envelope. The sheath contracts and pushes a harpoon into the neighboring cell envelope. Attached to that are effectors that cut peptidoglycan, damage membranes, or attack nucleic acids. Each effector has a cognate immunity protein in the attacker (periplasm or cytosol, depending on where the toxin acts), so the self is protected whilst neighbors without matching immunity are not <span class="citation" data-cites="kleinContactDependentInterbacterialAntagonism2020">(Klein, Ahmad, and Whitney 2020)</span>.</p>
<p><strong>Type IV secretion system:</strong> this is the conjugation / DNA-transfer machine used in plasmid transfer. In an antagonism context, the donor uses the same channel at the contact site to move toxic proteins or DNA directly into the recipient. Who gets targeted depends on which cells the T4SS can dock to, and what payload is being delivered. Again, killing only happens at close contact because the transfer channel has to form between the two cells <span class="citation" data-cites="kleinContactDependentInterbacterialAntagonism2020">(Klein, Ahmad, and Whitney 2020)</span>.</p>
<p><strong>Type VII secretion system (Gram-positives, T7SSb):</strong> a cluster of membrane proteins (Ess/Yuk, etc.) forms the export machinery. It secretes small toxins and larger toxins. These larger toxins act on nearby Gram-positive cells (membrane, cell wall, other essential targets). The toxin genes usually sit right next to a small immunity gene; if you carry both, you’re protected; if you’re a close relative missing that cassette, you’re at risk <span class="citation" data-cites="kleinContactDependentInterbacterialAntagonism2020">(Klein, Ahmad, and Whitney 2020)</span>.</p>
<p>At this point, it’s only natural to ask why there are multiple secretion systems. There’s the evolutionary answer: different lineages started from different “starting parts” (phage tails, conjugation systems, autotransporters) and solved the same problem in parallel. But cell envelope structure matters too: Gram-negatives with an outer membrane can support a T6SS; Gram-positives use T7SS instead. On top of that, the ecology is different: sometimes you want a very specific receptor-based hit (Type V), sometimes a more general protein-injection system (T6, T7), sometimes you care more about moving DNA than just poisoning (T4). Each system has its own range, targets, and style of immunity, so bacteria end up with a set of tools that cover different situations. That is, these aren’t superfluous mechanisms of antagonism <span class="citation" data-cites="willeyPrescottsMicrobiology2020">(Willey et al. 2020)</span>.</p>
<p>Interestingly, there’s a paper that shows a proof-of-concept on the implementation of secretion systems. Yim and Wang show how one can repurpose these weapons and actually use them on specified bacterial communities <span class="citation" data-cites="yimExploitingInterbacterialAntagonism2021">(Yim and Wang 2021)</span>. The actual paper referenced within Yim and Wang showed an engineered T6SS that uses nanobody binders (small, engineered antibody fragments) to focus killing of specific strains, among other reinventions <span class="citation" data-cites="tingTargetedDepletionBacteria2020">(Ting et al. 2020)</span>. One might find this confusing since T6SS does not require specific antibodies for killing under normal circumstances, as explained. The antibody on the attacker binds to the antigen on the target so they stay glued together long enough for the T6SS to fire into the target, instead of whoever happens to be nearby. The killing assays of E. coli were on the order of 570-fold and 220-fold with antibody targeting, versus only ~6-fold and ~3-fold when they used a control strain without antibody targeting.</p>
<p>At this point, it seems like a good idea to implement this within clinical trials to fight infection, but before that, there are several important considerations to make (these are merely inferences).</p>
<p>If the antigen being targeted were even partially expressed by commensals, one would run the risk of killing these beneficial species off by proxy. Also, dropping an engineered species into a host chock-full of bacteria is not the greatest idea, considering the risk of hypervirulence via mechanisms like HGT.</p>




<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-kleinContactDependentInterbacterialAntagonism2020" class="csl-entry">
Klein, Timothy A., Shehryar Ahmad, and John C. Whitney. 2020. <span>“Contact-<span>Dependent Interbacterial Antagonism Mediated</span> by <span>Protein Secretion Machines</span>.”</span> <em>Trends in Microbiology</em> 28 (5): 387–400. <a href="https://doi.org/10.1016/j.tim.2020.01.003">https://doi.org/10.1016/j.tim.2020.01.003</a>.
</div>
<div id="ref-tingTargetedDepletionBacteria2020" class="csl-entry">
Ting, See-Yeun, Esteban Martínez-García, Shuo Huang, Savannah K. Bertolli, Katherine A. Kelly, Kevin J. Cutler, Elizabeth D. Su, et al. 2020. <span>“Targeted <span>Depletion</span> of <span>Bacteria</span> from <span>Mixed Populations</span> by <span>Programmable Adhesion</span> with <span>Antagonistic Competitor Cells</span>.”</span> <em>Cell Host &amp; Microbe</em> 28 (2): 313–321.e6. <a href="https://doi.org/10.1016/j.chom.2020.05.006">https://doi.org/10.1016/j.chom.2020.05.006</a>.
</div>
<div id="ref-willeyPrescottsMicrobiology2020" class="csl-entry">
Willey, Joanne M., Lansing M. Prescott, Kathleen M. Sandman, and Dorothy H. Wood. 2020. <em>Prescott’s Microbiology</em>. Eleventh edition. New York, NY: McGraw-Hill Education.
</div>
<div id="ref-yimExploitingInterbacterialAntagonism2021" class="csl-entry">
Yim, Sung Sun, and Harris H. Wang. 2021. <span>“Exploiting Interbacterial Antagonism for Microbiome Engineering.”</span> <em>Current Opinion in Biomedical Engineering</em> 19 (September): 100307. <a href="https://doi.org/10.1016/j.cobme.2021.100307">https://doi.org/10.1016/j.cobme.2021.100307</a>.
</div>
</div></section></div> ]]></description>
  <category>Biology</category>
  <guid>https://jinsko.com/blog/sec_systems.html</guid>
  <pubDate>Fri, 19 Dec 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Lecanemab in Early Alzheimer’s Disease</title>
  <link>https://jinsko.com/blog/AD_paper.html</link>
  <description><![CDATA[ 




<p>The paper is a bit old, but it’s interesting; I learn from reading and writing about it so here’s my attempt.</p>
<p>The lecanemab paper reads, at first glance, like the trial we’ve been waiting for <span class="citation" data-cites="dyckLecanemabEarlyAlzheimers2023">(Dyck et al. 2023)</span>.</p>
<p>This is a big phase 3 trial in NEJM where an anti-amyloid antibody finally does what the field has been promising for quite some time. It strips amyloid out of the brain and the treatment group declines more slowly than placebo over eighteen months. It’s statistically clean, biomarkers all pointing in the <em>right</em> direction, figures that slope apart in the way grant proposals like to appeal.</p>
<p>Technically you are removing material; structurally it might be too late.</p>
<p>The core biology is pretty familiar. Amyloid-β is shaved off the amyloid precursor protein, misfolds, and aggregates into extracellular plaques. Tau, inside the neuron, becomes hyperphosphorylated and tangles up the microtubules that were supposed to carry cargo; synapses thin out; networks lose coherence <span class="citation" data-cites="kumarRobbinsCotranPathologic2021">(Kumar et al. 2021)</span>. In parallel, the vasculature of an aging brain quietly turns hostile: small-vessel disease, and crucially, CAA not in the parenchyma this time, but packed into the walls of tiny arteries and arterioles.</p>
<p>Lecanemab is a monoclonal antibody against soluble amyloid protofibrils, given as an intravenous infusion every two weeks. In this trial, about 1,800 people with “early” Alzheimer’s—mild cognitive impairment or mild dementia, all amyloid-positive on PET or CSF—were randomized to drug or placebo and followed for eighteen months. The primary outcome was change in the Clinical Dementia Rating Sum of Boxes (CDR-SB), a clinician-rated 0–18 scale that tries to compress memory, orientation, judgment, community affairs, hobbies, and personal care into a single trajectory.</p>
<p>On that metric, the placebo group worsened by about 1.66 points. The lecanemab group worsened by about 1.21. The difference—roughly 0.45 CDR-SB points (95% confidence interval, −0.67 to −0.23; P&lt;0.001)—is what becomes the “27% slowing of decline”. Amyloid PET scans show the drug is doing what it says: plaque signal falls dramatically compared with placebo. Fluid biomarkers like phosphorylated tau drift in the right direction. At the level of graphs and p-values, the paper is satisfyingly coherent.</p>
<p>But you have to translate these results back into an actual elderly human being.</p>
<p>Real patients are not the stylized creatures of trial diagrams. They come with AFib and a history of deep vein thrombosis and coronary stents and that one “small stroke” no one talks about at family dinners. They are already on warfarin or a direct oral anticoagulant to keep clots from forming in AFib; or on aspirin and clopidogrel to keep a metal stent in the left anterior descending from occluding; or both. These drugs flatten the normal peaks of coagulation so that clots are slower to form, bleeds are harder to stop, and any damage, even slight, to a vessel wall has more room to become a crisis.</p>
<p>The new class of anti-amyloid antibodies all carry the side effect, ARIA. ARIA-E is vasogenic edema or effusion, pockets of swelling or fluid that show in white on the MRI. ARIA-H is microhemorrhages that leave scars. In this trial, roughly a quarter of patients had infusion reactions, and around twelve to thirteen percent developed ARIA-E; ARIA-H also increased in the active arm. Most of these findings were asymptomatic and discovered on scheduled scans, but not all. Some patients had headache, confusion, or seizures; post-marketing reports have documented rare but real fatal hemorrhages. The FDA label now comes with a boxed warning spelling out the risks, especially in people on anticoagulants.</p>
<p>None of this is surprising. Anti-amyloid antibodies strip amyloid off vessel walls as well as off plaques. If those vessels are already stiff, hypertensive, and studded with amyloid from decades of cerebral amyloid angiopathy, “cleaning” them can mean destabilizing them. The wall leaks a little: ARIA-E. The wall cracks a little: ARIA-H. Add systemic anticoagulation or aggressive antiplatelet therapy to that fragility and the margin between “tiny microbleed that never matters” and “lobar hemorrhage that changes the course of your life” gets uncomfortably thin.</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart TD

    A[Elderly patient&lt;br/&gt;Vascular fragility] --&gt; B[Anticoagulants&lt;br/&gt;/ Antiplatelets]
    A --&gt; C[Lecanemab]

    C --&gt; D[Amyloid removal]
    D --&gt; H[Modest slowing&lt;br/&gt;of decline]

    D --&gt; E[ARIA-E / ARIA-H]
    B --&gt; E
    E --&gt; F[Edema / Microbleeds / ICH]
    F --&gt; G[Hospitalization&lt;br/&gt;Functional decline]

    H -. small benefit vs risk .- G
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>The trial tries to manage this with MRIs: baseline imaging to exclude anyone with too many microbleeds and follow-up scans at defined time points. This schedule alone is a selection pressure: to even begin, you need a body that tolerates lying still in a noisy tube at these points in time. Once you move from the idealized trial environment to a real clinic that already can’t get outpatients into MRI within a month, the whole premise is undermined.</p>
<p>Meanwhile the patient is supposed to show up every two weeks, indefinitely, for an intravenous infusion. For someone in their forties, this is boring. For someone in their eighties who walks with a frame and gets disoriented on unfamiliar floors, every visit is an opportunity for a fall, a urinary infection, a bout of hospital delirium that sets them back more than any fraction of a CDR-SB point can capture (remember that this is not a perfect metric).</p>
<p>There is also the quieter question of what “0.45 points” buys you, existentially.</p>
<p>The paper, reads like a proof of concept for a pathophysiological story, not a compelling offer for the average person who actually has this disease. It tells us that amyloid matters, that if you remove a lot of it early enough you can nudge the clinical trajectory in the right direction. It also tells us, indirectly, that by the time the diagnosis is even on the table, the brain is already woven through with other forms of damage that an antibody cannot touch.</p>




<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-dyckLecanemabEarlyAlzheimers2023" class="csl-entry">
Dyck, Christopher H. van, Chad J. Swanson, Paul Aisen, Randall J. Bateman, Christopher Chen, Michelle Gee, Michio Kanekiyo, et al. 2023. <span>“Lecanemab in <span>Early Alzheimer</span>’s <span>Disease</span>.”</span> <em>New England Journal of Medicine</em> 388 (1): 9–21. <a href="https://doi.org/10.1056/NEJMoa2212948">https://doi.org/10.1056/NEJMoa2212948</a>.
</div>
<div id="ref-kumarRobbinsCotranPathologic2021" class="csl-entry">
Kumar, Vinay, Abul K. Abbas, Jon C. Aster, Jerrold R. Turner, James Alfred Perkins, Stanley L. Robbins, and Ramzi S. Cotran, eds. 2021. <em>Robbins &amp; <span>Cotran</span> Pathologic Basis of Disease</em>. Tenth edition. Philadelphia, PA: Elsevier.
</div>
</div></section></div> ]]></description>
  <category>Medicine</category>
  <guid>https://jinsko.com/blog/AD_paper.html</guid>
  <pubDate>Thu, 11 Dec 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Diabetic Ketoacidosis and Non-Ketotic Coma</title>
  <link>https://jinsko.com/blog/diabetic-ketoacidosis.html</link>
  <description><![CDATA[ 




<p>Robbins pathology <span class="citation" data-cites="kumarRobbinsCotranPathologic2021">(Kumar et al. 2021)</span> and GH medical physiology <span class="citation" data-cites="hallGuytonHallTextbook2011">(Hall, Hall, and Guyton 2011)</span></p>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>Type 1 diabetes mellitus (T1DM) is an autoimmune disease characterized by the destruction of beta cells, leaving patients lacking insulin. Immune effector cells react to self-antigens present on the surface of beta cells. There are a variety of reasons why the effector cells target endogenous antigens (T-cell selection, HLA alleles, etc.), but that discussion would digress from the topic.</p>
<p>Type 2 diabetes mellitus (T2DM), on the other hand, is characterized by the decreased ability of peripheral tissues to respond to insulin (aka insulin resistance).</p>
<p>In summary, while T1DM is caused by the inability to produce insulin, T2DM is caused by decreased sensitivity to insulin. Both cause hyperglycemia.</p>
</section>
<section id="the-classic-triad" class="level1">
<h1>The Classic Triad</h1>
<p>Insulin is a fairly important anabolic hormone, so we’d expect to see various consequences in its absence or with decreased sensitivity. The classic triad is defined within the context of diabetes mellitus as: polyuria, polydipsia, polyphagia.</p>
<p>These manifestations of the condition can be explained starting with glycogenolysis, which is shared between the two types. Since the lack of insulin/insensitivity causes a decrease in glucose assimilation, storage of glycogen within the liver and muscle ceases and glycogenolysis happens instead. Glycogenolysis raises blood glucose, which eventually exceeds the renal threshold for glucose reabsorption from the filtrate to the blood. With excess glucose remaining in the filtrate (glycosuria), osmotic diuresis ensues. The diuresis then goes on to manifest itself as <em>polyuria</em>. The osmotic diuresis and hyperosmotic blood lead to overall water loss. This triggers osmoreceptors in the brain, hence <em>polydipsia</em>. Since glucose is also unavailable to peripheral tissues, the body resorts to catabolism of proteins and fats. This leads to an increase in appetite, <em>polyphagia</em>.</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">sequenceDiagram
    participant A as Lack of insulin/insensitivity
    participant B as Liver &amp; muscle
    participant C as Blood glucose
    participant D as Kidney
    participant E as Brain osmoreceptors
    participant F as Peripheral tissues

    A-&gt;&gt;B: ↓ glucose assimilation
    B-&gt;&gt;B: glycogen storage ceases
    B-&gt;&gt;B: glycogenolysis
    B-&gt;&gt;C: raises glucose
    C-&gt;&gt;D: exceeds renal threshold
    D-&gt;&gt;D: glycosuria
    D-&gt;&gt;D: osmotic diuresis
    D-&gt;&gt;D: polyurea
    D-&gt;&gt;E: water loss + hyperosmotic blood
    E-&gt;&gt;E: polydipsia
    A-&gt;&gt;F: glucose unavailable
    F-&gt;&gt;F: protein/fat catabolism
    F-&gt;&gt;F: increased appetite (polyphagia)
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</section>
<section id="the-metabolic-consequences" class="level1">
<h1>The Metabolic Consequences</h1>
<section id="t1dm" class="level2">
<h2 class="anchored" data-anchor-id="t1dm">T1DM</h2>
<p>Deficiency of insulin leads to activation of lipase in adipose stores, releasing free fatty acids (FFAs). The liver then oxidizes FFAs, turning them into ketones, which can be used for energy in the absence of glucose. This is called ketogenesis. If the rate of ketogenesis exceeds the rate of consumption by peripheral tissues, ketones accumulate in the blood (ketonemia), leading to metabolic acidosis.</p>
<p>Metabolic acidosis is often accompanied by an increase in respiratory rate to blow off CO₂, which helps raise blood pH. It’s interesting how insulin deficiency can ultimately lead to something that may seem completely unrelated at first.</p>
</section>
<section id="t2dm" class="level2">
<h2 class="anchored" data-anchor-id="t2dm">T2DM</h2>
<p>Ketoacidosis doesn’t occur very frequently in T2DM, since these patients still have enough insulin to inhibit ketogenesis—and hence acidosis. Sustained osmotic diuresis in the absence of metabolic acidosis can cause hyperosmolar nonketotic coma. Because symptoms like rapid respiration don’t appear, it’s hard to recognize the severity of this condition until the late stages of dehydration, especially if the patient is already impaired and fails to hydrate. The hyperosmolarity eventually draws water out of brain cells, potentially causing coma.</p>



</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-hallGuytonHallTextbook2011" class="csl-entry">
Hall, John E., John E. Hall, and Arthur C. Guyton. 2011. <em>Guyton and <span>Hall</span> Textbook of Medical Physiology</em>. 12. ed. Philadelphia, Pa: Saunders, Elsevier.
</div>
<div id="ref-kumarRobbinsCotranPathologic2021" class="csl-entry">
Kumar, Vinay, Abul K. Abbas, Jon C. Aster, Jerrold R. Turner, James Alfred Perkins, Stanley L. Robbins, and Ramzi S. Cotran, eds. 2021. <em>Robbins &amp; <span>Cotran</span> Pathologic Basis of Disease</em>. Tenth edition. Philadelphia, PA: Elsevier.
</div>
</div></section></div> ]]></description>
  <category>Medicine</category>
  <guid>https://jinsko.com/blog/diabetic-ketoacidosis.html</guid>
  <pubDate>Wed, 24 Sep 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Overview Of Sequencing</title>
  <link>https://jinsko.com/blog/08-29-25.html</link>
  <description><![CDATA[ 




<p>I was reading <em>An Introduction to Genetic Engineering—4th</em> by Nicholl and wanted to write about the applications of the Klenow fragment but instead got distracted by a different subject altogether.</p>
<section id="st-gen-sequencing" class="level2">
<h2 class="anchored" data-anchor-id="st-gen-sequencing">1st Gen Sequencing</h2>
<p>Dideoxy gene sequencing uses what is called “primed synthesis” for sequencing. Primed synthesis works as follows: primers anneal onto a denatured strand of DNA, yielding ssDNA and creating a free 3’-OH group. A DNA polymerase then binds to the oligonucleotide primer, copying the remainder of the template strand. Understanding this mechanism is important for understanding the later stages of sequencing.</p>
<p>To sequence a specific section of the genome, the DNA sequence in question is cloned via PCR. This ensures the proper strength of the signal on the autoradiograph later on. The DNA duplex is then denatured, and primers are annealed. The Klenow fragment then synthesizes the copy of the template strand. The Klenow fragment is used instead of DNA polymerase I because the fragment lacks the 5’→3’ endonuclease function. This ensures the primer is not digested when the polymerase is introduced to the sample.</p>
<p>Within the test tubes, along with the dNTPs used to elongate the daughter strand, one type of ddNTP is included in low concentrations, hence the name “dideoxy” sequencing. The ddNTPs are missing the 3’-OH group, disallowing elongation of the strand as phosphodiester bonds aren’t allowed to form. The concentrations of ddNTPs are kept low to ensure that termination of the daughter strand does not happen too often. This is done four times in four different test tubes to create fragments that terminate at corresponding nucleotides.</p>
<p>As a result, nested fragments are formed. These are fragments of DNA that differ in length due to the random incorporation of ddNTPs. You can run these fragments along a four-channel agarose gel (each channel corresponds to a test tube with a specific ddNTP), usually with urea and high temperature to ensure denaturing conditions. You don’t want secondary structures to interfere with the distance traveled by the fragments.</p>
<p>You can then take the gel and expose it onto a film to create the autoradiograph. Reading this is pretty simple: just read the film from the bottom up. Keep your eyes peeled for minuscule differences between the base pairs across the four channels. First-generation sequencing is pretty accurate and is still used today; it is the gold standard of sequencing due to its accuracy.</p>
</section>
<section id="ngs" class="level2">
<h2 class="anchored" data-anchor-id="ngs">NGS</h2>
<p>NGS stands for next-generation sequencing.</p>
<p>After dideoxy sequencing was developed, large-scale sequencing consisted of large machines in increasingly larger buildings. This was obviously not very cost-effective, and scientists needed a different approach to sequencing.</p>
<p>While second-generation sequencing requires the amplification of identical fragments via PCR, third-gen sequencing does not, eliminating any errors that may arise at this stage. For both generations, you can use two types of gene libraries: <em>Fragment libraries</em>, which are produced via sonication of the genome, or <em>amplicon libraries</em>, which are produced via PCR of a certain subsection of the genome. Naturally, since you are targeting the entire genome, there is less coverage (less likelihood of sequencing a certain base pair) with fragment libraries. Fragment libraries are more probability-based than amplicon libraries since fragment libraries cover a larger area. To fix this, you could increase the depth of the read, but that’s more work. Under normal circumstances, one achieves what they need from an amplicon library that tells them something about a specific gene of interest or a subsection of the genome. The ends of the fragments are usually edited so that detection of the fragments becomes possible.</p>
<p>Third-gen sequencing is more interesting to me, so I’ll talk about that and skip second-gen, though I feel a bit bad (not really). This type of sequencing has absolutely no semblance to 1st-gen (Dideoxy sequencing).</p>
<section id="smrt-sequencing" class="level3">
<h3 class="anchored" data-anchor-id="smrt-sequencing">SMRT Sequencing</h3>
<p>SMRT stands for Single-Molecule Real-Time sequencing.</p>
<p>The contraption used for sequencing consists of a DNA polymerase attached to the bottom of zero-mode waveguides (very small wells), approximately 100 nm in diameter. A strand of ssDNA is then passed into the well, along with dNTP-fl (fluorophore-labeled dNTPs). From the bottom of the well, an excitatory wavelength is shot out, so when the ssDNA incorporates the dNTP-fl as its daughter strand, a wavelength of light is emitted specific to the nucleotide, allowing for sequencing. It helps to have the zero-mode waveguide so small to dampen any background noise.</p>
<p>Note that if you are passing a linear sequence of DNA through the ZMW, the accuracy tends to be pretty low. Instead, adaptors are added to the ends of the DNA to make the fragment of DNA circular. This allows for repetition of reads, increasing the depth of sequencing. You could technically just throw a linear sequence in, but accuracy goes down real fast; something like 99.9% to 87%.</p>
</section>
<section id="nanopore-sequencing" class="level3">
<h3 class="anchored" data-anchor-id="nanopore-sequencing">Nanopore Sequencing</h3>
<p>This is the most modern type of third-gen sequencing. The smallest version of this device can be held in your palm and be plugged into a computer. This particular piece of technology consists of a membrane with embedded nanopores, usually constructed of proteins. A voltage is applied across this polymer membrane, and a current is created in the nanopore. When an ssDNA strand passes through this nanopore, small disturbances in the current are detected. These disturbances can be discerned from each other due to the different sizes of the nitrogenous bases. A sequence can then be generated with the disturbances plotted on a graph.</p>
<p>An adaptor sequence is added onto the strand to improve entry into the nanopore. There’s usually a motor protein of some sort passing the ssDNA into the pore, something like helicase.</p>


</section>
</section>

 ]]></description>
  <category>Biology</category>
  <guid>https://jinsko.com/blog/08-29-25.html</guid>
  <pubDate>Fri, 29 Aug 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Mol/Micro Biology is Merely an Abstraction</title>
  <link>https://jinsko.com/blog/08-29-25(2).html</link>
  <description><![CDATA[ 




<p>I’ve had this thought for a while but have never bothered writing it down. I was reminded of it while I was writing about sequencing. I found myself making wishy-washy explanations, characteristic of a biologist, when it came to the mechanisms at the molecular level.</p>
<p>I can only explain these pieces of technology from the perspective of an amateur biologist, and would, instead, if possible, explain these technologies from the perspective of a chemist or physicist. But I am merely a biologist. It is my firm belief that after a certain point, biology is merely an abstraction of chemistry and physics, and a thorough understanding of these disciplines (which I do not possess) is required for a complete understanding. That is, my knowledge of these technologies will start breaking down after questions pertaining to the chemistry and physics of these technologies are brought forth. I would argue that one must be a decent physicist and chemist before even attempting to call themselves a good biologist.</p>
<p>The beauty of biology is only appreciated once you understand mechanism in totality.</p>
<p>If a biologist desires to go beyond conventional applications of a tool available to them, an understanding of how things work is crucial, and not just how to use them. But the understanding of <em>how things work</em> only arrives along with the knowledge of these superior disciplines that have generated the field we now call biology.</p>
<p>Since the amount of knowledge required to complete these novel experiments is far beyond the scope of a single biologist, one would expect to find biologists eager to collaborate with chemists and physicists alike. This is not uncommon, but far less common than I expect it to be, which I find very odd.</p>
<p>I apologize for the abrupt ending but I’ll stop there since further discussion will turn into a lengthy rant.</p>



 ]]></description>
  <category>Misc</category>
  <guid>https://jinsko.com/blog/08-29-25(2).html</guid>
  <pubDate>Fri, 29 Aug 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>How this Website was Made</title>
  <link>https://jinsko.com/blog/blog_post.html</link>
  <description><![CDATA[ 




<p>I used Quarto, which was recommended to me by a friend for its simplicity and ease of management. I could have used any number of other tools to build the website for greater customization, but I don’t sweat the details; the uncouth aesthetics of a website don’t bring me much displeasure. I’m also lazy, which I suppose is the real reason.</p>
<p>What I did to deploy my website is by no means special, so writing about this process is merely a way for me to fully understand it.</p>
<p>To customize the website to my needs (though it is still a bleak website and may eventually require an update), I made a few adjustments to the <code>styles.css</code> file, including—but not limited to—setting the font, making the TOC and navbar sticky, and adding page transition animations. As for the theme specified in the configuration file, I simply built the website off the “lightly” theme.</p>
<p>I hosted the website on GitHub Pages but bought a custom domain, so the DNS records needed to point to four <code>A</code> records that connect to GitHub’s servers. From there, a <code>CNAME</code> record points to my GitHub Page. Then, all I needed to do was tell GitHub that I had a custom domain ready.</p>
<p>Rather than publishing from the <code>main</code> branch, I set up a separate branch, <code>gh-pages</code>, and published from there. This keeps the repo clean: I can isolate the build artifacts in that branch and deploy from there, relieving the main branch of unnecessary clutter. <code>_site</code> is also included in <code>.gitignore</code> to reduce clutter on the main branch, ensuring that only actual edits are visible in the version history.</p>
<p>That concludes the mechanics behind the website. I’m sure I’ve mentioned this on the home page, but this website is primarily a way for me to put what I learn into words, which I hope will aid the learning process. I’ll continue doing this with other subjects of interest.</p>



 ]]></description>
  <category>CS</category>
  <guid>https://jinsko.com/blog/blog_post.html</guid>
  <pubDate>Thu, 21 Aug 2025 00:00:00 GMT</pubDate>
</item>
</channel>
</rss>
